{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PdJMjyenJR4o"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "References:\n",
        "https://pytorch.org/text/stable/datasets.html\n",
        "https://pytorch.org/text/_modules/torchtext/datasets/text_classification.html\n",
        "\n",
        "\"\"\"\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "from torchtext.datasets import SogouNews, YelpReviewPolarity\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "      super(TextClassificationModel, self).__init__()\n",
        "      self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "      self.fc = nn.Linear(embed_dim, num_class)\n",
        "      self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "      initrange = 0.5\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "      self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "      self.fc.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "      embedded = self.embedding(text, offsets)\n",
        "      return self.fc(embedded)\n",
        "\n",
        "class MainClass:  \n",
        "  def __init__(self, dataset_class=AG_NEWS):\n",
        "    self.model = None\n",
        "    self.dataset_class = dataset_class\n",
        "\n",
        "    train_iter = self.dataset_class(split = 'train')\n",
        "    self.vocab = build_vocab_from_iterator(self.yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "    self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "\n",
        "    self.text_pipeline = lambda x: self.vocab(tokenizer(x))\n",
        "    self.label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "  @staticmethod\n",
        "  def yield_tokens(data_iter):\n",
        "      for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "  def train(self, dataloader, epoch):\n",
        "      self.model.train()\n",
        "      total_acc, total_count = 0, 0\n",
        "      log_interval = 500\n",
        "      start_time = time.time()\n",
        "\n",
        "      for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "          self.optimizer.zero_grad()\n",
        "          predited_label = self.model(text, offsets)\n",
        "          loss = self.criterion(predited_label, label)\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
        "          self.optimizer.step()\n",
        "          total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "          total_count += label.size(0)\n",
        "          if idx % log_interval == 0 and idx > 0:\n",
        "              elapsed = time.time() - start_time\n",
        "              print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                    '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                                total_acc/total_count))\n",
        "              total_acc, total_count = 0, 0\n",
        "              start_time = time.time()\n",
        "\n",
        "  def evaluate(self, dataloader):\n",
        "      self.model.eval()\n",
        "      total_acc, total_count = 0, 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "              predited_label = self.model(text, offsets)\n",
        "              loss = self.criterion(predited_label, label)\n",
        "              total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "              total_count += label.size(0)\n",
        "      return total_acc/total_count\n",
        "\n",
        "\n",
        "  def collate_batch(self, batch):\n",
        "      label_list, text_list, offsets = [], [], [0]\n",
        "\n",
        "      for (_label, _text) in batch:\n",
        "          label_list.append(self.label_pipeline(_label))\n",
        "          processed_text = torch.tensor(self.text_pipeline(_text), dtype=torch.int64)\n",
        "          text_list.append(processed_text)\n",
        "          offsets.append(processed_text.size(0))\n",
        "\n",
        "      label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "      offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "      text_list = torch.cat(text_list)\n",
        "\n",
        "      return label_list.to(device), text_list.to(device), offsets.to(device)   \n",
        "\n",
        "\n",
        "  def train_and_validate(self):\n",
        "    train_iter = self.dataset_class(split='train')\n",
        "    num_class = len(set([label for (label, text) in train_iter]))\n",
        "    vocab_size = len(self.vocab)\n",
        "    emsize = 64\n",
        "    self.model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
        "      \n",
        "    self.criterion = torch.nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1.0, gamma=0.1)\n",
        "\n",
        "    total_accu = None\n",
        "    train_iter, test_iter = self.dataset_class()\n",
        "    train_dataset = to_map_style_dataset(train_iter)\n",
        "    test_dataset = to_map_style_dataset(test_iter)\n",
        "    num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "    split_train_, split_valid_ = \\\n",
        "        random_split(train_dataset, \n",
        "                    [num_train, len(train_dataset) - num_train]\n",
        "                    )\n",
        "\n",
        "    train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                                  shuffle=True, collate_fn=self.collate_batch)\n",
        "    valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                                  shuffle=True, collate_fn=self.collate_batch)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=self.collate_batch)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        self.train(train_dataloader, epoch)\n",
        "        accu_val = self.evaluate(valid_dataloader)\n",
        "        if total_accu is not None and total_accu > accu_val:\n",
        "          scheduler.step()\n",
        "        else:\n",
        "          total_accu = accu_val\n",
        "        print('-' * 59)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "              'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                              time.time() - epoch_start_time,\n",
        "                                              accu_val))\n",
        "        print('-' * 59)\n",
        "\n",
        "    print('Checking the results of test dataset.')\n",
        "    accu_test = self.evaluate(test_dataloader)\n",
        "    print('test accuracy {:8.3f}'.format(accu_test))\n",
        "  \n",
        "  def predict(self, text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = self.model(text, torch.tensor([0]))\n",
        "        print(output)\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "  def predict_ouput(self, input_text, labels_dict):\n",
        "    self.model = self.model.to(\"cpu\")\n",
        "    print(\"This is a %s news\" %labels_dict[self.predict(input_text, self.text_pipeline)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06eLIfc1siyK",
        "outputId": "956f8a44-e2b7-43ca-cc29-3fc2ccc09d1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sogou_news_csv.tar.gz: 100%|██████████| 384M/384M [00:03<00:00, 110MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 6680 batches | accuracy    0.824\n",
            "| epoch   1 |  1000/ 6680 batches | accuracy    0.907\n",
            "| epoch   1 |  1500/ 6680 batches | accuracy    0.916\n",
            "| epoch   1 |  2000/ 6680 batches | accuracy    0.920\n",
            "| epoch   1 |  2500/ 6680 batches | accuracy    0.923\n",
            "| epoch   1 |  3000/ 6680 batches | accuracy    0.925\n",
            "| epoch   1 |  3500/ 6680 batches | accuracy    0.926\n",
            "| epoch   1 |  4000/ 6680 batches | accuracy    0.927\n",
            "| epoch   1 |  4500/ 6680 batches | accuracy    0.926\n",
            "| epoch   1 |  5000/ 6680 batches | accuracy    0.928\n",
            "| epoch   1 |  5500/ 6680 batches | accuracy    0.926\n",
            "| epoch   1 |  6000/ 6680 batches | accuracy    0.930\n",
            "| epoch   1 |  6500/ 6680 batches | accuracy    0.928\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 175.49s | valid accuracy    0.931 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  1000/ 6680 batches | accuracy    0.927\n",
            "| epoch   2 |  1500/ 6680 batches | accuracy    0.928\n",
            "| epoch   2 |  2000/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  2500/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  3000/ 6680 batches | accuracy    0.934\n",
            "| epoch   2 |  3500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  4000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  4500/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  5000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  5500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  6000/ 6680 batches | accuracy    0.936\n",
            "| epoch   2 |  6500/ 6680 batches | accuracy    0.933\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 175.26s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  1000/ 6680 batches | accuracy    0.932\n",
            "| epoch   3 |  1500/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  2000/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  2500/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  3000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  3500/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  4000/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  4500/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  5000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  5500/ 6680 batches | accuracy    0.937\n",
            "| epoch   3 |  6000/ 6680 batches | accuracy    0.936\n",
            "| epoch   3 |  6500/ 6680 batches | accuracy    0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 173.38s | valid accuracy    0.934 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  1000/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  1500/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  2000/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  2500/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  3000/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  3500/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  4000/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  4500/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  5000/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  5500/ 6680 batches | accuracy    0.934\n",
            "| epoch   4 |  6000/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  6500/ 6680 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 174.80s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  1000/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  1500/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  2000/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  2500/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  3000/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  3500/ 6680 batches | accuracy    0.942\n",
            "| epoch   5 |  4000/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  4500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  5000/ 6680 batches | accuracy    0.942\n",
            "| epoch   5 |  5500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  6000/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  6500/ 6680 batches | accuracy    0.940\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 176.32s | valid accuracy    0.937 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.936\n",
            "tensor([[-0.7924, -2.5487,  0.0276,  7.4774, -4.2892]])\n",
            "This is a Automobile news\n"
          ]
        }
      ],
      "source": [
        "main_class = MainClass(SogouNews)\n",
        "main_class.train_and_validate()\n",
        "main_class.predict_ouput(\n",
        "    input_text = '2008 di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n me3i nv3 mo2 te4  2008di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n yu2 15 ri4 za4i qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n she4ng da4 ka1i mu4 . be3n ci4 che1 zha3n jia1ng chi2 xu4 da4o be3n yue4 19 ri4 . ji1n nia2n qi1ng da3o guo2 ji4 che1 zha3n shi4 li4 nia2n da3o che2ng che1 zha3n gui1 mo2 zui4 da4 di2 yi1 ci4 , shi3 yo4ng lia3o qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n di2 qua2n bu4 shi4 ne4i wa4i zha3n gua3n . yi3 xia4 we2i xia4n cha3ng mo2 te4 tu2 pia4n .',\n",
        "    labels_dict = {\n",
        "      1: 'Sports',\n",
        "      2: 'Finance',\n",
        "      3: 'Entertainment',\n",
        "      4: 'Automobile',\n",
        "      5: 'Technology'}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ala9P-eMUIaj",
        "outputId": "af1ee4a3-1fd8-49a8-c85b-b62b74ac953e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.781\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.862\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.877\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.889\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.895\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.897\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.901\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.903\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.902\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.905\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.909\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.912\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.911\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 81.39s | valid accuracy    0.922 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.913\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.915\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.913\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.920\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.920\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.918\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 82.42s | valid accuracy    0.924 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.918\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.920\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 84.57s | valid accuracy    0.904 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.935\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.932\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.936\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.935\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.936\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.936\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.936\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.935\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.937\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.932\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.936\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.935\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 81.72s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.938\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.938\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.937\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.938\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.937\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.938\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 81.27s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.936\n",
            "tensor([[ 2.4543, -2.4812]])\n",
            "This is a Negative polarity news\n"
          ]
        }
      ],
      "source": [
        "main_class = MainClass(YelpReviewPolarity)\n",
        "main_class.train_and_validate()\n",
        "main_class.predict_ouput(\n",
        "    input_text = \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\",\n",
        "    labels_dict = {\n",
        "      1: 'Negative polarity',\n",
        "      2: 'Positive polarity',\n",
        "      }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Session5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
