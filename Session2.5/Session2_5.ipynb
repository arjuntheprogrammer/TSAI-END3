{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session2.5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcaKN0qQ4LYi"
      },
      "source": [
        "# TSAI Assignment\n",
        "\n",
        "## Session 2.5 - PyTorch 101\n",
        "\n",
        "1. Write a neural network that\n",
        "   a. can take 2 inputs:\n",
        "\n",
        "   1. an image from the MNIST dataset (say 5), and\n",
        "   2. a random number between 0 and 9, (say 7)\n",
        "\n",
        "   b. and gives 2 outputs:\n",
        "\n",
        "   1. the \"number\" that was represented by the MNIST image (predict 5), and\n",
        "   2. the \"sum\" of this number with the random number and the input image to the network (predict 5 + 7 = 12)\n",
        "\n",
        "2. You can mix fully connected layers and convolution layers\n",
        "\n",
        "3. You can use one-hot encoding to represent the random umber input as well as the \"summed\" output.\n",
        "\n",
        "4. **Your code MUST be** - well documented (via readme file on github and comments in the code) - must mention the data representation - must mention your data generation strategy (basically the class/method you are using for random number generation) - must mention how you have combined the two inputs (basically which layer you are combining) - must mention how you are evaluating your results - must mention \"what\" results you finally got and how did you evaluate your results - must mention what loss function you picked and why!\n",
        "\n",
        "5. Training MUST happen on the PU\n",
        "\n",
        "6. Accuracy is not really important for the UM\n",
        "\n",
        "7. Once done, upload the code with short trining logs in the readme file from colab to GitHub, and share the GitHub lik (public repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxg1SA1G2Ldw"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import os\n",
        "os.makedirs('/results/', exist_ok =True)\n",
        "\n",
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n26ntod7B3dl"
      },
      "source": [
        "def get_binary(n):\n",
        "    return [1 if(each>0.5) else 0 for each in n]\n",
        "\n",
        "class CustomRandDataset(Dataset):\n",
        "  def __init__(self, mnist_data, is_train=True):\n",
        "    if(is_train):\n",
        "      self.rand_number_data = torch.randint(low=0, high=9, size=(60000, ))\n",
        "    else:\n",
        "      self.rand_number_data = torch.randint(low=0, high=9, size=(10000, ))\n",
        "    \n",
        "    self.mnist_data = mnist_data\n",
        "\n",
        "    # prepare one hot encoded - data & labels\n",
        "    self.encoded_rand_number_data = F.one_hot(self.rand_number_data, num_classes=10)\n",
        "\n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "\n",
        "    for ((mnist_data, mnist_label), rand_number) in tqdm(zip(self.mnist_data, self.rand_number_data)):\n",
        "      encoded_rand_number = F.one_hot(rand_number, num_classes=10)\n",
        "      self.data.append(torch.cat((torch.tensor(mnist_data), encoded_rand_number), 0))\n",
        "      \n",
        "      sum = mnist_label + rand_number\n",
        "      encoded_mnist_label = F.one_hot(torch.tensor(mnist_label), num_classes=10)\n",
        "      encoded_sum = F.one_hot(sum, num_classes=19)\n",
        "      self.labels.append(torch.cat((encoded_mnist_label, encoded_sum), 0))\n",
        "\n",
        "    print(\"len(self.data) = \", len(self.data))\n",
        "    print(\"len(self.labels) = \", len(self.labels))\n",
        "  \n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index], self.labels[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "mnist_train_dataset = torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                               transform=torchvision.transforms.Compose([\n",
        "                                   torchvision.transforms.ToTensor(),\n",
        "                                  T.Lambda(lambda x: torch.flatten(x)),\n",
        "                                  T.Lambda(lambda x: np.array(get_binary(x))),\n",
        "                                  ]))\n",
        "\n",
        "mnist_test_dataset = torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                               transform=torchvision.transforms.Compose([\n",
        "                                   torchvision.transforms.ToTensor(),\n",
        "                                  T.Lambda(lambda x: torch.flatten(x)),\n",
        "                                  T.Lambda(lambda x: np.array(get_binary(x))),\n",
        "                               ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(CustomRandDataset(mnist_data=mnist_train_dataset, is_train=True), \n",
        "                                                batch_size=batch_size_train, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(CustomRandDataset(mnist_data=mnist_test_dataset, is_train=False), \n",
        "                                               batch_size = batch_size_test, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPVT-MkLmz5y",
        "outputId": "e3206f45-c755-4c44-f09c-6058a2c213c5"
      },
      "source": [
        "for x,y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "  print(y[0])\n",
        "  break\n",
        "\n",
        "for x,y in test_loader:\n",
        "  print(x.shape,y.shape)\n",
        "  break"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 794]) torch.Size([64, 29])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0])\n",
            "torch.Size([1000, 794]) torch.Size([1000, 29])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GChdcZx3hzaZ"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, num_classes=29):\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "        self.layer = nn.Sequential(\n",
        "            # in_channels=794, out_channels=1, kernel_size=3, stride=1\n",
        "            nn.Conv1d(in_channels=in_channels, out_channels=1, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(1, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(1, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(1, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(1, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "        ) \n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "        self.fc = nn.Linear(25, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer(x)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EtxWKvTRqHd",
        "outputId": "0dfc42ae-0c10-4ecc-e272-cbb711db33a9"
      },
      "source": [
        "network = Network()\n",
        "print(network)\n",
        "\n",
        "optimizer = optim.SGD(network.parameters(), \n",
        "                      lr=learning_rate,\n",
        "                      momentum=momentum)\n",
        "lossF = nn.BCELoss()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    network.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        data = torch.unsqueeze(data, 1).type(torch.float)\n",
        "        output = network(data)\n",
        "        new_output = torch.tensor([[1 if element>0.5 else 0 for element in each] for each in output], dtype=torch.float)\n",
        "        loss = lossF(output, target.type(torch.float))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            train_losses.append(loss.item())\n",
        "            train_counter.append(\n",
        "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "            torch.save(network.state_dict(), '/results/model.pth')\n",
        "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
        "\n",
        "def test():\n",
        "    network.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = torch.unsqueeze(data, 1).type(torch.float)\n",
        "            output = network(data)\n",
        "            new_output = torch.tensor([[1 if element>0.5 else 0 for element in each] for each in output], dtype=torch.float)\n",
        "            test_loss += lossF(output, target.type(torch.float)).item()\n",
        "\n",
        "            for x,y in zip(new_output, target):\n",
        "              if(all(torch.eq(x, y))):\n",
        "                correct += 1\n",
        "\n",
        "    print(correct)\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (layer): Sequential(\n",
            "    (0): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
            "    (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
            "    (4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
            "    (7): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
            "    (10): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=25, out_features=29, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.728947\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.715486\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.703058\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.683889\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.674063\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.662374\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.653794\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.634777\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.622374\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.606739\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.587893\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.570010\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.548769\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.529923\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.508585\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.481409\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.457115\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.432274\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.414690\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.387108\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.362782\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.342769\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.327613\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.313967\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.300809\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.291279\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.286975\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.277130\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.268984\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.265485\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.263526\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.260266\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.258013\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.255311\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.252831\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.252057\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.251799\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.252270\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.249749\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.252755\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.248273\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.248556\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.247920\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.251888\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.247051\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.245175\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.246363\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.246706\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.249149\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.244509\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.246196\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.246122\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.243239\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.248101\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.245742\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.245313\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.245778\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.245058\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.244118\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.245016\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.245623\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.243084\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.240978\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.244206\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.243044\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.245059\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.245143\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.242054\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.241806\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.246327\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.243210\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.244571\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.245794\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.249284\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.246300\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.240808\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.242253\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.248251\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.241933\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.240782\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.241111\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.245246\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.241109\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.238107\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.241853\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.241939\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.241957\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.242492\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.239492\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.240898\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.237086\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.241531\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.242790\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.240135\n",
            "0\n",
            "\n",
            "Test set: Avg. loss: 0.0002, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.241774\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.237467\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.241543\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.241438\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.241501\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.244783\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.242263\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.241699\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.242228\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.239381\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.240846\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.238822\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.236829\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.240094\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.238526\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.237677\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.241545\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.242530\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.243473\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.239271\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.239541\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.239761\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.242515\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.241120\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.239976\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.237163\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.242630\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.240700\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.239922\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.244819\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.240645\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.241996\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.239554\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.239150\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.238679\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.240561\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.239990\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.241856\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.238757\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.243852\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.239637\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.241641\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.236625\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.245112\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.239869\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.238120\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.239988\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.238811\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.241782\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.238264\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.240506\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.238332\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.236332\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.241335\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.238714\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.239446\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.239741\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.240387\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.239121\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.240434\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.240176\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.236749\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.236429\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.238380\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.238642\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.240144\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.242131\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.237301\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.236609\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.241660\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.238536\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.241238\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.241588\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.245013\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.241874\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.235366\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.237371\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.245276\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.237769\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.236251\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.237218\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.241425\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.236910\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.234426\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.238096\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.237338\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.237671\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.238905\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.235538\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.236824\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.232790\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.237864\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.237142\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.236526\n",
            "0\n",
            "\n",
            "Test set: Avg. loss: 0.0002, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.238201\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.233010\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.238004\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.237461\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.237682\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.241460\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.238219\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.237057\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.238864\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.235419\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.237658\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.235868\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.233105\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.236102\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.233788\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.234511\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.237815\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.239554\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.240079\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.236176\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.236005\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.235679\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.239053\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.237848\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.237108\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.234161\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.239073\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.236865\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.235920\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.241510\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.236634\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.238476\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.235761\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.235711\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.234937\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.237386\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.236891\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.238532\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.234847\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.239973\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.235869\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.238111\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.233536\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.241906\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.237176\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.234151\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.236420\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.236150\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.239012\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.235309\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.237238\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.234392\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.232597\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.238092\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.235155\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.236048\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.236510\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.237205\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.235501\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.237132\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.236187\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.232752\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.233149\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.234430\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.235504\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.236271\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.239337\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.233649\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.233209\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.238928\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.234433\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.237759\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.237943\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.241229\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.238428\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.231809\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.233874\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.241593\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.234214\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.232584\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.234025\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.237655\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.232867\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.230859\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.234456\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.233181\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.233797\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.235029\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.231791\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.233044\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.228907\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.233969\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.232651\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.232176\n",
            "0\n",
            "\n",
            "Test set: Avg. loss: 0.0002, Accuracy: 0/10000 (0%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "VOkcAdAEer56",
        "outputId": "7c461acf-6aeb-46a8-bded-93d73eb2af77"
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "# plt.scatter(test_counter, test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'negative log likelihood loss')"
            ]
          },
          "metadata": {},
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqffxCA3grFK"
      },
      "source": [
        "\n",
        "# fig = plt.figure()\n",
        "# for i in range(6):\n",
        "#     plt.subplot(2, 3, i+1)\n",
        "#     plt.tight_layout()\n",
        "#     plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "#     plt.title(\"Prediction: {}\".format(\n",
        "#         output.data.max(1, keepdim=True)[1][i].item()))\n",
        "#     plt.xticks([])\n",
        "#     plt.yticks([])\n",
        "# fig\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}