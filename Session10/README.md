# TSAI Assignment

## SESSION 10 - Transformers Review

ASSIGNMENT

1. Train the same code, but on different data. If you have n-classes, your accuracy MUST be more than 4 \* 100 / n.
2. Submit the Github link, that includes your notebook with training logs, and proper readme file.

---

## DATASET USED

MANYTHINGS ORG DUTCH - ENGLISH DATASET

Links:

- <https://www.manythings.org/anki/>
- <https://www.manythings.org/anki/nld-eng.zip>
- [nld-eng.zip](https://github.com/arjuntheprogrammer/TSAI-END3/files/7882966/nld-eng.zip)

![image](https://user-images.githubusercontent.com/15984084/149796986-fb678ac6-60d1-4983-a81b-5e53721c25f5.png)

---

## DIAGRAMS

### Transformer

![Transformer](assets/transformer.png)

### Encoder

![Encoder](assets/encoder.png)

### Attention

![Transformer](assets/attention.png)

### Decoder

![Transformer](assets/decoder.png)

---

## SCREENSHOTS

### TRAINING LOGS

![image](https://user-images.githubusercontent.com/15984084/149768903-52d9f715-3887-47db-8fd9-760366d4a36a.png)

### EVALUATION OUTPUT

![image](https://user-images.githubusercontent.com/15984084/149768990-848f1725-51e0-442d-8732-c11fc6e352ef.png)

### TRANSLATION OUTPUT

![image](https://user-images.githubusercontent.com/15984084/149769186-02f7db3c-7a24-47e4-89ea-87176dab9189.png)

## REFERENCES

1. Attention is All You Need: <https://github.com/ammesatyajit/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb>
2. Paper: Attention is All You Need <https://arxiv.org/pdf/1706.03762.pdf>
3. The Illustrated Transformer
   <https://jalammar.github.io/illustrated-transformer/>
4. What Do Position Embeddings Learn? <https://arxiv.org/pdf/2010.04903.pdf>
5. <https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb>

---
