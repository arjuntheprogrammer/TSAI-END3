{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebupkScS1xsl"
      },
      "outputs": [],
      "source": [
        "!wget http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz\n",
        "!tar -xvf  Question_Answer_Dataset_v1.2.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoX6sO6r2iUw",
        "outputId": "3ef669df-288d-45c8-cba1-fc8ce087bb14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines:  S08/\n",
            "Reading lines:  S09/\n",
            "Reading lines:  S10/\n",
            "\n",
            "Read 3998 sentence pairs\n",
            "Trimmed to 2113 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "ques 1993\n",
            "ans 1367\n",
            "\n",
            "\n",
            "random.choice(pairs) = \n",
            " ['is romania a secular state ?', 'yes']\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "from torch import optim\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readFile(input_name, output_name):\n",
        "    pairs = []\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    base_dir = '/content/Question_Answer_Dataset_v1.2/'\n",
        "    folder_names = ['S08/', 'S09/', 'S10/']\n",
        "    file_name = 'question_answer_pairs.txt'\n",
        "    \n",
        "    for folder_name in folder_names:\n",
        "      print(\"Reading lines: \", folder_name)\n",
        "      lines = open(\n",
        "        base_dir + folder_name + file_name,\n",
        "        encoding='utf-8',\n",
        "        errors='ignore'\n",
        "      ).read().strip().split('\\n')\n",
        "\n",
        "      # Split every line into pairs and normalize\n",
        "      pairs.extend(\n",
        "          [[normalizeString(s) for s in l.split('\\t')[1:3]] for l in lines[1:]]\n",
        "      )\n",
        "\n",
        "    input_question = Lang(input_name)\n",
        "    output_answer = Lang(output_name)\n",
        "\n",
        "    return input_question, output_answer, pairs\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "def filterPair(p):\n",
        "    return (len(p[0].split(' ')) < MAX_LENGTH and \n",
        "        len(p[1].split(' ')) < MAX_LENGTH)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "def prepareData(input_name, output_name):\n",
        "    input_question, output_answer, pairs = readFile(input_name, output_name)\n",
        "    print(\"\\nRead %s sentence pairs\" % len(pairs))\n",
        "    \n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    \n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_question.addSentence(pair[0])\n",
        "        output_answer.addSentence(pair[1])\n",
        "    \n",
        "    print(\"Counted words:\")\n",
        "    print(input_question.name, input_question.n_words)\n",
        "    print(output_answer.name, output_answer.n_words)\n",
        "    return input_question, output_answer, pairs\n",
        "\n",
        "\n",
        "input_question, output_answer, pairs = prepareData('ques', 'ans')\n",
        "print(\"\\n\\nrandom.choice(pairs) = \\n\", random.choice(pairs))\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_question, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_answer, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, \n",
        "          encoder_optimizer, decoder_optimizer, criterion, \n",
        "          max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(\n",
        "        max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_question, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(\n",
        "            max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_answer.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "\n",
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ts1RIWh9RIT",
        "outputId": "06cfae03-821b-484e-994d-ccb6f884f42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1m 12s (- 16m 49s) (5000 6%) 2.3753\n",
            "2m 18s (- 15m 0s) (10000 13%) 1.9309\n",
            "3m 26s (- 13m 46s) (15000 20%) 1.5232\n",
            "4m 34s (- 12m 35s) (20000 26%) 1.1007\n",
            "5m 43s (- 11m 27s) (25000 33%) 0.8352\n",
            "6m 53s (- 10m 20s) (30000 40%) 0.6706\n",
            "8m 4s (- 9m 13s) (35000 46%) 0.5027\n",
            "9m 14s (- 8m 5s) (40000 53%) 0.4431\n",
            "10m 25s (- 6m 56s) (45000 60%) 0.4101\n",
            "11m 36s (- 5m 48s) (50000 66%) 0.3634\n",
            "12m 47s (- 4m 38s) (55000 73%) 0.3505\n",
            "13m 57s (- 3m 29s) (60000 80%) 0.3485\n",
            "15m 7s (- 2m 19s) (65000 86%) 0.3253\n",
            "16m 18s (- 1m 9s) (70000 93%) 0.3142\n",
            "17m 28s (- 0m 0s) (75000 100%) 0.3138\n"
          ]
        }
      ],
      "source": [
        "######################################################\n",
        "\n",
        "hidden_size = 256\n",
        "\n",
        "encoder1 = EncoderRNN(input_question.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(\n",
        "    hidden_size, output_answer.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "\n",
        "######################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNA-ySGoYpjA",
        "outputId": "f03336f7-93b2-44e9-8e65-6ce7c67f19ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> where was james monroe born ?\n",
            "= westmoreland county virginia\n",
            "< westmoreland county virginia <EOS>\n",
            "\n",
            "> how has canada helped un peacekeeping efforts ?\n",
            "= null\n",
            "< null <EOS>\n",
            "\n",
            "> what are zebras hunted for ?\n",
            "= mainly for their skins\n",
            "< skins <EOS>\n",
            "\n",
            "> did wilson support desegregation ?\n",
            "= not in the slightest .\n",
            "< not in the slightest . <EOS>\n",
            "\n",
            "> what are the three dialect regions of vietnamese ?\n",
            "= null\n",
            "< the three regions are north central and south . <EOS>\n",
            "\n",
            "> is crissy field an airfield\n",
            "= yes\n",
            "< yes <EOS>\n",
            "\n",
            "> who became adversaries with nikola tesla ?\n",
            "= nikola tesla became adversaries with edison .\n",
            "< edison <EOS>\n",
            "\n",
            "> how many counties is romania divided into ?\n",
            "=  .\n",
            "< forty one <EOS>\n",
            "\n",
            "> what is the national language of singapore ?\n",
            "= the national language of singapore is malay .\n",
            "< the national language of singapore is malay . <EOS>\n",
            "\n",
            "> is arabic classified alongside semitic languages ?\n",
            "= yes\n",
            "< null <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYwtMP_-Aboa",
        "outputId": "33b4dcc4-87ce-4dc5-8720-4266c0000e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input = did lincoln ever represent alton sangamon railroad ?\n",
            "output = yes <EOS>\n",
            "input = what is named after him ?\n",
            "output = the celsius crater on the moon <EOS>\n",
            "input = are certain species of beetles considered pests ?\n",
            "output = yes <EOS>\n",
            "input = how many long was lincoln s formal education ?\n",
            "output =  months . <EOS>\n"
          ]
        }
      ],
      "source": [
        "######################################################\n",
        "\n",
        "evaluateAndShowAttention(\"did lincoln ever represent alton sangamon railroad ?\")\n",
        "evaluateAndShowAttention(\"what is named after him ?\")\n",
        "evaluateAndShowAttention(\"are certain species of beetles considered pests ?\")\n",
        "evaluateAndShowAttention(\"how many long was lincoln s formal education ?\")\n",
        "\n",
        "######################################################"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Session6-Dataset1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
